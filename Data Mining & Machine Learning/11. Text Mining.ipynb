{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7869f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of content\n",
    "\n",
    "# text processing\n",
    "# document similarity based on TF-IDF\n",
    "# document similarity based on LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf29e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  ['credit', 'card', 'fraud', 'detect', 'import', 'credit', 'card', 'compani', 'abl', 'recogn', 'fraudul', 'credit', 'card', 'transact', 'custom', 'charg', 'item', 'purchas', 'dataset', 'contain', 'transact', 'credit', 'card', 'septemb', 'european', 'cardhold', 'dataset', 'present', 'transact', 'occur', 'dai', 'fraud', 'transact', 'dataset', 'highli', 'unbalanc', 'posit', 'class', 'fraud', 'account', 'transact', 'contain', 'numer', 'input', 'variabl', 'result', 'pca', 'transform', 'unfortun', 'confidenti', 'issu', 'provid', 'origin', 'featur', 'background', 'inform', 'data', 'featur', 'princip', 'compon', 'obtain', 'pca', 'featur', 'transform', 'pca', 'time', 'featur', 'time', 'contain', 'second', 'elaps', 'transact', 'transact', 'dataset', 'featur', 'transact', 'featur', 'exampl', 'depend', 'cost', 'senstiv', 'learn', 'featur', 'class', 'respons', 'variabl', 'take', 'valu', 'case', 'fraud']\n",
      "test texts:  ['health', 'insur', 'marketplac', 'public', 'us', 'file', 'contain', 'data', 'health', 'dental', 'plan', 'offer', 'individu', 'small', 'busi', 'health', 'insur', 'marketplac', 'help', 'start', 'data', 'explor', 'idea', 'plan', 'rate', 'benefit', 'vari', 'state', 'plan', 'benefit', 'relat', 'plan', 'rate', 'plan', 'rate', 'vari', 'ag', 'plan', 'vari', 'insur', 'network', 'provid', 'data', 'origin', 'prepar', 'releas', 'center', 'medicar', 'medicaid', 'servic', 'cm', 'read', 'cm', 'disclaim', 'user', 'agreement', 'data']\n"
     ]
    }
   ],
   "source": [
    "# text processing ############################\n",
    "\n",
    "from gensim import corpora, models, similarities,matutils\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "# load data\n",
    "# each line is text description of each project\n",
    "with open('ProjectTexts.txt', 'r', encoding='utf-8') as f:\n",
    "    train = []\n",
    "    for line in f.readlines():\n",
    "        # preprocessing, such as lower case, stopword removal, stemming\n",
    "        # return a list of stemmed terms\n",
    "        line=preprocess_string(line)\n",
    "        train.append(line)\n",
    "        \n",
    "print('training data: ',train[0])\n",
    "\n",
    "# build a dictionary\n",
    "dictionary = corpora.Dictionary(train)\n",
    "\n",
    "# load test text\n",
    "with open('ProjectTexts_TestText.txt', 'r', encoding='utf-8')as fr:\n",
    "    line = fr.readline()\n",
    "    test=preprocess_string(line)\n",
    "    test_vec = dictionary.doc2bow(test)\n",
    "print('test texts: ',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b31fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total terms:  1289\n",
      "corpus size: 50\n",
      "TF-IDF file saved.\n",
      "TF in doc1: [(0, 1), (1, 1), (2, 1), (3, 4), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 3), (12, 1), (13, 4), (14, 1), (15, 1), (16, 1), (17, 4), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 7), (24, 4), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 8), (52, 2), (53, 1), (54, 1), (55, 1), (56, 2)]\n",
      "TF-idf in doc1 [(0, 0.04730669241449516), (1, 0.043560889562723706), (2, 0.08037265141657753), (3, 0.23120649190054587), (4, 0.08037265141657753), (5, 0.043560889562723706), (6, 0.05780162297513647), (7, 0.13226383600832953), (8, 0.04730669241449516), (9, 0.06613191800416476), (10, 0.08037265141657753), (11, 0.07022915330077939), (12, 0.05780162297513647), (13, 0.26452767201665905), (14, 0.06613191800416476), (15, 0.04039385837753037), (16, 0.006749127708869872), (17, 0.050638264369017374), (18, 0.08037265141657753), (19, 0.06613191800416476), (20, 0.08037265141657753), (21, 0.05780162297513647), (22, 0.05189118459175201), (23, 0.2827570086427126), (24, 0.3214906056663101), (25, 0.08037265141657753), (26, 0.05780162297513647), (27, 0.04039385837753037), (28, 0.02473566397305409), (29, 0.06613191800416476), (30, 0.05780162297513647), (31, 0.06613191800416476), (32, 0.04730669241449516), (33, 0.08037265141657753), (34, 0.04730669241449516), (35, 0.06613191800416476), (36, 0.04039385837753037), (37, 0.2411179542497326), (38, 0.05780162297513647), (39, 0.03765045117933923), (40, 0.08037265141657753), (41, 0.03765045117933923), (42, 0.05780162297513647), (43, 0.08037265141657753), (44, 0.05189118459175201), (45, 0.05189118459175201), (46, 0.05780162297513647), (47, 0.08037265141657753), (48, 0.08037265141657753), (49, 0.05780162297513647), (50, 0.058640312300621875), (51, 0.6429812113326202), (52, 0.13226383600832953), (53, 0.08037265141657753), (54, 0.06613191800416476), (55, 0.05189118459175201), (56, 0.07530090235867846)]\n",
      "\n",
      "Similarity between test text and others:\n",
      " [(0, 0.01543166), (1, 0.004794034), (2, 0.02182176), (3, 0.0043771104), (4, 0.058692977), (5, 0.03222183), (6, 0.0077693835), (7, 0.013135217), (8, 0.007940551), (9, 0.016108934), (10, 0.013376986), (11, 0.035562243), (12, 0.029964335), (13, 0.0), (14, 0.0), (15, 0.024796627), (16, 0.03003384), (17, 0.0), (18, 0.012692177), (19, 0.028156847), (20, 0.029653285), (21, 0.0052406155), (22, 0.033039127), (23, 0.02976758), (24, 0.03612104), (25, 0.009384365), (26, 0.023124605), (27, 0.043578688), (28, 0.021402135), (29, 0.111121245), (30, 0.024953758), (31, 0.0), (32, 0.03567668), (33, 0.039689288), (34, 0.0), (35, 0.015223809), (36, 0.05516596), (37, 0.02592308), (38, 0.031529997), (39, 0.032050535), (40, 0.010222848), (41, 0.012347261), (42, 0.04586076), (43, 0.0075283237), (44, 0.90744424), (45, 0.066232085), (46, 0.111635245), (47, 0.015049451), (48, 0.01579918), (49, 0.048092555)]\n",
      "\n",
      "Similarity between doc0 and others:\n",
      " [(0, 0.9365669), (1, 0.021528795), (2, 0.032298833), (3, 0.0007767878), (4, 0.04040096), (5, 0.04815425), (6, 0.020623043), (7, 0.032095432), (8, 0.0136676), (9, 0.051707514), (10, 0.054861933), (11, 0.046549387), (12, 0.025266627), (13, 0.013143216), (14, 0.023177616), (15, 0.011093197), (16, 0.030803578), (17, 0.004814457), (18, 0.05228766), (19, 0.026623446), (20, 0.010376919), (21, 0.045082215), (22, 0.009281796), (23, 0.023777561), (24, 0.050301403), (25, 0.017031213), (26, 0.042129397), (27, 0.012398487), (28, 0.027954888), (29, 0.012177265), (30, 0.04905495), (31, 0.11566573), (32, 0.013519623), (33, 0.020040102), (34, 0.010421744), (35, 0.021009307), (36, 0.023088912), (37, 0.03894071), (38, 0.044489797), (39, 0.0072440235), (40, 0.007431714), (41, 0.05280085), (42, 0.020598348), (43, 0.04960354), (44, 0.013633697), (45, 0.030384071), (46, 0.030376533), (47, 0.007564962), (48, 0.024230238), (49, 0.0025797256)]\n",
      "\n",
      "Similarity between test text and others (top-10):\n",
      " [(44, 0.9074442386627197), (46, 0.1116352453827858), (29, 0.11112124472856522), (45, 0.06623208522796631), (4, 0.05869297683238983), (36, 0.055165961384773254), (49, 0.0480925552546978), (42, 0.045860759913921356), (27, 0.04357868805527687), (33, 0.039689287543296814)]\n"
     ]
    }
   ],
   "source": [
    "# document similarity by using TF-IDF weighted matrix ############################\n",
    "\n",
    "\n",
    "count = len(dictionary.token2id)  # number of unique terms\n",
    "print('total terms: ',count)\n",
    "dictionary.save('dict.txt')  # save dict\n",
    "# build corpus using bag of words (bow)\n",
    "corpus_tf = [dictionary.doc2bow(text) for text in train]\n",
    "print('corpus size:',len(corpus_tf))\n",
    "# build tf-idf model\n",
    "corpus_tfidf = models.TfidfModel(corpus_tf)\n",
    "# save tf-idf data\n",
    "with open('ProjectTexts_TFIDF.txt', 'w', encoding='utf-8') as fr:\n",
    "    for doc in corpus_tfidf[corpus_tf]:\n",
    "        # each line is TF-IDF vector for a document\n",
    "        # the vector only saves the terms in the document\n",
    "        fr.write(doc.__str__() + '\\n')\n",
    "    print('TF-IDF file saved.')\n",
    "    \n",
    "# print first term-frequency of first line\n",
    "print('TF in doc1:',corpus_tf[0])\n",
    "# print tf-idf of first line\n",
    "print('TF-idf in doc1',corpus_tfidf[corpus_tf[0]])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.similarities.docsim import MatrixSimilarity\n",
    "\n",
    "# get simialarities\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf[corpus_tf], num_features=count)\n",
    "# get simiarlity between the test docs, and all docs in train\n",
    "sims = index[test_vec]\n",
    "print('\\nSimilarity between test text and others:\\n',list(enumerate(sims)))\n",
    "\n",
    "doc0=corpus_tf[0]\n",
    "sims = index[doc0]\n",
    "print('\\nSimilarity between doc0 and others:\\n',list(enumerate(sims)))\n",
    "\n",
    "# get top-10 best similar documents\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf[corpus_tf], num_features=count, num_best=10)\n",
    "sims = index[test_vec]\n",
    "print('\\nSimilarity between test text and others (top-10):\\n',sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93037b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Print topics:\n",
      " [(0, '0.005*\"user\" + 0.005*\"anim\" + 0.004*\"transact\" + 0.004*\"chicago\" + 0.004*\"gym\" + 0.004*\"homicid\" + 0.004*\"iri\" + 0.004*\"french\" + 0.003*\"predict\" + 0.003*\"featur\"'), (1, '0.003*\"metacrit\" + 0.003*\"activ\" + 0.003*\"elect\" + 0.003*\"sale\" + 0.003*\"game\" + 0.003*\"candid\" + 0.003*\"correl\" + 0.003*\"download\" + 0.003*\"republican\" + 0.003*\"observ\"'), (2, '0.006*\"sale\" + 0.004*\"deputi\" + 0.004*\"movi\" + 0.003*\"epidem\" + 0.003*\"zika\" + 0.003*\"menu\" + 0.003*\"actor\" + 0.003*\"director\" + 0.003*\"nutrit\" + 0.003*\"game\"'), (3, '0.005*\"appoint\" + 0.005*\"postcod\" + 0.005*\"visa\" + 0.004*\"feder\" + 0.004*\"happi\" + 0.004*\"return\" + 0.003*\"flood\" + 0.003*\"rate\" + 0.003*\"tax\" + 0.003*\"risk\"'), (4, '0.005*\"emot\" + 0.004*\"homicid\" + 0.004*\"physic\" + 0.003*\"murder\" + 0.003*\"datasheet\" + 0.003*\"chang\" + 0.003*\"perpetr\" + 0.002*\"project\" + 0.002*\"victim\" + 0.002*\"relationship\"'), (5, '0.005*\"contest\" + 0.005*\"voic\" + 0.005*\"review\" + 0.005*\"mushroom\" + 0.004*\"world\" + 0.004*\"indic\" + 0.004*\"amazon\" + 0.003*\"bachelor\" + 0.003*\"develop\" + 0.003*\"new\"'), (6, '0.006*\"loan\" + 0.005*\"termin\" + 0.004*\"crime\" + 0.004*\"circadian\" + 0.004*\"free\" + 0.004*\"throw\" + 0.003*\"flight\" + 0.003*\"neuron\" + 0.003*\"fluoresc\" + 0.003*\"employe\"'), (7, '0.005*\"plan\" + 0.003*\"insur\" + 0.003*\"air\" + 0.003*\"steam\" + 0.003*\"plai\" + 0.003*\"behavior\" + 0.003*\"game\" + 0.003*\"valu\" + 0.003*\"summari\" + 0.003*\"delai\"'), (8, '0.005*\"pipelin\" + 0.004*\"analyt\" + 0.004*\"india\" + 0.003*\"event\" + 0.003*\"employe\" + 0.003*\"breast\" + 0.003*\"wise\" + 0.003*\"censu\" + 0.003*\"district\" + 0.003*\"footbal\"'), (9, '0.005*\"trump\" + 0.004*\"vote\" + 0.004*\"date\" + 0.004*\"uber\" + 0.003*\"speed\" + 0.003*\"climat\" + 0.003*\"temperatur\" + 0.003*\"drive\" + 0.003*\"mile\" + 0.002*\"particip\"')]\n",
      "\n",
      "Print topic-vector for doc0:\n",
      " [(0, 0.8532128), (1, 0.016309887), (2, 0.016310913), (3, 0.016308011), (4, 0.016307665), (5, 0.016308127), (6, 0.016308011), (7, 0.016315093), (8, 0.016308531), (9, 0.016311005)]\n",
      "\n",
      "Similarity between test text and others:\n",
      " [(0, 0.14021698), (1, 0.030847607), (2, 0.14231165), (3, 0.034748852), (4, 0.019798078), (5, 0.02587526), (6, 0.14886935), (7, 0.016031366), (8, 0.14687526), (9, 0.020947732), (10, 0.13777941), (11, 0.013741794), (12, 0.10393609), (13, 0.028583352), (14, 0.032691352), (15, 0.01835562), (16, 0.021466311), (17, 0.029360086), (18, 0.13875768), (19, 0.13557458), (20, 0.037990905), (21, 0.02130791), (22, 0.101096004), (23, 0.015512709), (24, 0.103000656), (25, 0.033821866), (26, 0.0143692605), (27, 0.10168134), (28, 0.022307198), (29, 0.016505707), (30, 0.99160403), (31, 0.1447453), (32, 0.021240383), (33, 0.11295378), (34, 0.032117255), (35, 0.020931505), (36, 0.023795804), (37, 0.02467932), (38, 0.020625982), (39, 0.016087167), (40, 0.03593347), (41, 0.014462106), (42, 0.13928948), (43, 0.019832717), (44, 0.99166685), (45, 0.016667586), (46, 0.15329191), (47, 0.11433103), (48, 0.9915465), (49, 0.025015457)]\n",
      "\n",
      "Similarity between doc0 and others:\n",
      " [(0, 1.0), (1, 0.048832048), (2, 0.99998194), (3, 0.05258282), (4, 0.03818883), (5, 0.044050474), (6, 0.9996904), (7, 0.03455787), (8, 0.999817), (9, 0.039300255), (10, 0.9999759), (11, 0.032360345), (12, 0.041824494), (13, 0.04664984), (14, 0.05061695), (15, 0.036804933), (16, 0.039811898), (17, 0.047411703), (18, 0.9999914), (19, 0.9999129), (20, 0.05573285), (21, 0.039646126), (22, 0.038856268), (23, 0.034055173), (24, 0.04089735), (25, 0.051689122), (26, 0.03296757), (27, 0.03946292), (28, 0.040616475), (29, 0.03501244), (30, 0.04081379), (31, 0.99991554), (32, 0.039574154), (33, 0.051301513), (34, 0.05006037), (35, 0.039281543), (36, 0.04203953), (37, 0.04289176), (38, 0.03899372), (39, 0.03460951), (40, 0.053727772), (41, 0.03305206), (42, 0.99999654), (43, 0.038235586), (44, 0.04479161), (45, 0.035174727), (46, 0.9992895), (47, 0.052748594), (48, 0.039160654), (49, 0.04321146)]\n",
      "\n",
      "Similarity between test text and others (top-10):\n",
      " [(0, 0.1412808895111084), (43, 0.07397810369729996), (46, 0.0169710461050272), (31, 0.010146964341402054), (32, 0.008192148059606552), (27, 0.007353533990681171), (41, 0.0060273027047514915), (24, 0.0028500936459749937)]\n"
     ]
    }
   ],
   "source": [
    "# document similarity by using LDA ############################\n",
    "\n",
    "corpus_tfidfdata=corpus_tfidf[corpus_tf]\n",
    "# build LDA based on 10 topics\n",
    "num_topics=10\n",
    "lda = models.LdaModel(corpus_tfidfdata, id2word=dictionary, num_topics=10)\n",
    "print('\\nPrint topics:\\n',lda.print_topics(num_topics))\n",
    "\n",
    "# represent documents by using the distribution over topics\n",
    "corpus_doc2topic=lda[corpus_tfidfdata]\n",
    "print('\\nPrint topic-vector for doc0:\\n',corpus_doc2topic[0])\n",
    "\n",
    "# get simialarities\n",
    "index = similarities.MatrixSimilarity(corpus_doc2topic, num_features=num_topics)\n",
    "# get simiarlity between the test docs, and all docs in train\n",
    "test_vec_lda=lda[test_vec]\n",
    "sims = index[test_vec_lda]\n",
    "print('\\nSimilarity between test text and others:\\n',list(enumerate(sims)))\n",
    "\n",
    "doc0=corpus_doc2topic[0]\n",
    "sims = index[doc0]\n",
    "print('\\nSimilarity between doc0 and others:\\n',list(enumerate(sims)))\n",
    "\n",
    "# get top-10 best similar documents\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf[corpus_tf], num_features=count, num_best=10)\n",
    "sims = index[test_vec_lda]\n",
    "print('\\nSimilarity between test text and others (top-10):\\n',sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb3b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
