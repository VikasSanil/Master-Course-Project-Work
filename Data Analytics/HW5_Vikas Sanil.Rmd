---
title: "HW4_Vikas Sanil"
author: "Vikas Sanil"
date: "Due 4/20 11:59 pm"
output:
  pdf_document: default
  '': default
header-includes:
- \usepackage{float}
- \floatplacement{figure}{H}
fig_cap: yes
keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
   echo = TRUE, 
   fig.align = 'center' , 
   out.width="80%"
)
```

### GRADING

* Part I = 20 points;
* Part II = 80 points;



# Part I: Review of basic concepts in statistical learning (20 points)

You will spend some time thinking of some real-life applications for statistical learning. 

### Question 1. 
Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. 

>Answer: Three real-life applications in which classification useful.   
1. Type of project based on number of functional requirement, number of department involved, cost of project, duration estimated, estimated hour, estimated price, previous actual time taken, previous actual hour, previous actual price. This is an inference example as project type is inferred based on past experience in similar project.   
2. Credit worthiness(response) classification based on age, demography, job type, income and credit score(predictors).This is an inference example where credit worthiness is inferred based on similar profile of other customers.   
3. Malware(response) classifiction based on new/emerging malwares on the basis of comparable features like delivery system, data delivered, data compramised, commnication and system control(predictors). This ia an inference example as the malware is classified based on similarity between other existing malwares.


### Question 2. 
Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

> Answer: Three real-life applications in which regression anlysis useful.   
1. Weather(response) forecasting based on above ground temperature, wind, water vapour density, below ground temp, sunlight, cloud density, landscape and ocean/river water level(predictors). The goal of this is to predict weather for the future based on past data.   
2. Time(response) required to loose certain weight based on age, gender, body mass, calories intake, calories burnt and current weight. This is a prediction example where the time required to loose certain weight is predicted based on data available.   
3. Women Ovulation period(response) based on women period cycle, age, pH level and body temperature(predictors). This is a prediction example as women ovulation period is predicted on the similar set of data.

### Question 3.
Describe three real-life applications in which cluster analysis might be useful.

> Answer: Three real-life applications in which cluster analysis is useful.   
1. Advertisement(response) placement in browser based on user age, gender, demography, and past 5 browsing topics(predictors). The goal of this example is to infer an advertisement a user may like based on past online activities.   
2. Investment product(response) suggestion to a new investment banking client based on existing customer age, gender, demography, credit score, average balance(predictors), and preferred investment product. The goal of this application is to predict the best-suited investment product for an investment banking client based on existing customer details.   
3. Restaurant(response) suggestion in Food app. based on member age, gender, demography, and past 5 cuisine selections(predictors). The goal of this application is to infer user restaurant interest based on past choices along with user details.

  
### Question 4. 
What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

> Answer:   
- Flexible approach provides more coverage of data. Less flexible approach provides easy to understand relationship.
- When inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. When interested in prediction and interpretability is not required the most flexible model works. 

# Part II: Multiple Linear Regression (80 points)

Load the `Boston` data set

```{r}
# import packages
library(MASS)
library(MLmetrics)
library(AICcmodavg)
#load data
data(Boston)
```


## Exploratory data analysis (10 points)

* Check the number of observations and features using `dim`

```{r}
str(Boston)
dim(Boston)
```

> Answer: There are 506 observations and 14 features in Boston data set.

* Check for missing values

```{r}
which(is.na(Boston))
sum(is.na(Boston))
```

> Answer: There are no missing values.

* Check for duplicated values

```{r}
sum(duplicated(Boston))
```

> Answer: There are no duplicated values.

* checking correlation between variables 

```{r}
res<-cor(Boston)
round(res,2)
```

>Answer: tax~ rad-> 0.91 has highest correlation between variables.


## Split data set into 80:20 train and test data with name `BostonTraining` and `BostonTesting` respectively (10 points)

```{r}
i <- sample(2, nrow(Boston), replace=TRUE, prob=c(0.8, 0.2))
BostonTraining <- Boston[i==1,]
BostonTest <- Boston[i==2,]
```


## Subset Selection Linear Regression Model

### Forward Stepwise (25 points)

* Please construct a forward stepwise regression with `BostonTraining`.

```{r}
#null model
intercept_only<-lm(medv~ 1, data=BostonTraining)
#  full model
all<-lm(medv~., data = BostonTraining)
# forward set-wise regression
forward<- stepAIC(intercept_only, direction='forward', scope=formula(all))
#results
forward$anova
summary(forward)
```

> Answer: Final forward model with optimal set of features is lm(medv~lstat+rm+ptratio+dis+nox+chas+black+zn+crim+rad+tax, data=BostonTraining ) 

* Use this model to predict `medv` in `BostonTesting` and calculate `MAE` and `MSE`.

```{r}
ypred_forward<-predict(object = forward, newdata=BostonTest)
MAE(y_pred = ypred_forward, y_true = BostonTest$medv)
MSE(y_pred = ypred_forward, y_true = BostonTest$medv)
```

>Answer:   
- The MAE is 3.28 and MSE is 17.40 for forward model on BostonTraining data set.   
- The forecast distance between true value will be 3.28. 
- The model is not perfect.

### Backward Stepwise (25 points)

* Please construct a backward stepwise regression with `BostonTraining`.

```{r}
# backward set-wise regression
backward<- stepAIC(all, direction='backward')
#results
backward$anova
summary(backward)
```

>Answer: Final forward model with optimal set of features is lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat, data=BostonTraining )

* Use this model to predict `medv` in `BostonTesting` and calculate `MAE` and `MSE`.

```{r}
ypred_backward<-predict(object = backward, newdata=BostonTest)
MAE(y_pred = ypred_backward, y_true = BostonTest$medv)
MSE(y_pred = ypred_backward, y_true = BostonTest$medv)
```

>Answer:   
- The MAE is 3.28 and MSE is 17.40 for forward model on BostonTraining data set.   
- The forecast distance between true value will be 3.28. 
- The model is not perfect.

## Model Assessment (10 points)

Compare the forward and backward stepwise linear regression models. You can use plots, assessment measures ($R^2$, RSS, MAE, MSE,etc.) Which one is better? Explain your answer.

```{r}
par(mfrow=c(2,2))
plot(forward)
par(mfrow=c(2,2))
plot(backward)

par(mfrow=c(1,2))
summary(forward)$coefficients
summary(backward)$coefficients

anova(forward,backward)

modelset<-list(forward,backward)
aictab(modelset)
```

>Answer: Both model have same result. Hence both are better.

